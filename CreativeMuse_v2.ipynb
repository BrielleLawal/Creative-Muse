{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQVan1fusJSSVzT+YqpCG5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Installing and Importing Libraries"
      ],
      "metadata": {
        "id": "KeggkIIdwKQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gradio"
      ],
      "metadata": {
        "id": "tNmJDIhGObH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNBdcrN_v_NQ"
      },
      "outputs": [],
      "source": [
        "!pip install -q google-genai langchain langchain-google-genai langchain-core langchain-community chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from IPython.display import display, Markdown, HTML\n",
        "from google.colab import userdata\n",
        "from google import genai\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import base64\n",
        "import os"
      ],
      "metadata": {
        "id": "GQ6OGBtdxBIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "GOOGLE_API_KEY = getpass.getpass(\"Enter your Google API Key: \")\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "print(\"Key succesfully configured\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ggpko2W4xBF7",
        "outputId": "4b17219b-e45a-45e8-ae3f-1953a69704c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Google API Key: ··········\n",
            "Key succesfully configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for model in client.models.list():\n",
        "#     if \"gemini\" in model.name.lower():\n",
        "#         print(f\"Model name: {model.name}\")\n",
        "#         print(f\"Display name: {model.display_name}\")\n",
        "#         print(f\"Description: {model.description}\")\n",
        "#         print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "tmZ-w91LxBEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash-lite\",\n",
        "    contents=\"What is life?\")\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I8pteYUHxBCW",
        "outputId": "c18399ed-5d41-4ba3-9ee6-cd47685ade3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "That's the ultimate question, isn't it? Defining \"life\" is surprisingly complex, and the answer depends on what you're looking for: a scientific definition, a philosophical one, or something else entirely. Here's a breakdown from different perspectives:\n\n**1. The Scientific Perspective (Biology):**\n\nScientists generally agree on a set of characteristics that define life.  These are often summarized using the acronym **MRS GREN**:\n\n*   **M**etabolism:  The ability to obtain and use energy and nutrients from the environment.  This involves chemical reactions to build complex molecules (anabolism) and break down molecules for energy (catabolism).\n*   **R**esponse to Stimuli:  The ability to react to changes in the environment (e.g., light, temperature, touch, chemicals).\n*   **S**elf-regulation (Homeostasis):  The ability to maintain a stable internal environment despite external changes. (e.g., regulating body temperature, blood sugar, pH).\n*   **G**rowth: An increase in size and complexity, often by cell division and production of new materials.\n*   **R**eproduction:  The ability to create new organisms, either sexually or asexually.\n*   **E**xcretion: The process of removing waste products from the body.\n*   **N**utrition: The process of taking in nutrients from the environment to fuel metabolism.\n\n**Key Concepts within the Scientific Definition:**\n\n*   **Cells:**  The fundamental unit of life.  All known living organisms are made of one or more cells.  Cells are the smallest units that can perform all the functions of life.\n*   **DNA/RNA:**  Genetic material that carries the instructions for building and operating an organism.  This allows for inheritance and evolution.\n*   **Evolution:** Life evolves over time through natural selection, driven by changes in DNA and environmental pressures.\n\n**Important Considerations for the Scientific Definition:**\n\n*   **Viruses:**  Viruses blur the lines. They have genetic material and can reproduce, but they need a host cell to do so. They don't exhibit all the characteristics of life on their own, which is why some scientists don't consider them to be alive.\n*   **The search for life beyond Earth:** Scientists are constantly looking for life on other planets and moons. Defining \"life\" clearly helps them identify potential signs of life.\n\n**2. The Philosophical Perspective:**\n\nPhilosophy delves into the meaning and purpose of life. This is much more subjective and considers concepts like:\n\n*   **Consciousness:** The state of awareness, sentience, and self-awareness.\n*   **Purpose:**  What is the reason for existence?  Do humans have a preordained purpose, or do they create their own?\n*   **Meaning:** What gives life significance? Is it relationships, accomplishments, experiences, or something else?\n*   **Mortality:** The inevitability of death. How does this impact the meaning and value of life?\n*   **Free will vs. determinism:**  Do we have control over our actions, or are they predetermined?\n\n**Key Philosophers and Their Thoughts:**\n\n*   **Existentialism (Sartre, Camus):** Life is inherently meaningless, and humans are free to create their own meaning. Responsibility for our choices is emphasized.\n*   **Nihilism:** Life is without objective meaning, purpose, or intrinsic value.\n*   **Absurdism (Camus):** The conflict between the human desire for meaning and the meaningless universe.  Accepting the absurd allows for a sense of liberation.\n*   **Hedonism:**  The pursuit of pleasure and avoidance of pain is the highest good.\n*   **Stoicism:**  Virtue and reason are the key to happiness, and accepting what you can't control.\n\n**3. The Spiritual/Religious Perspective:**\n\nMany religions offer answers to the question of life, often involving:\n\n*   **A creator or higher power:**  Life may be seen as a gift from a divine being.\n*   **A soul or spirit:** A non-physical part of a person that survives death.\n*   **Purpose and morality:**  Life is often seen as having a specific purpose, such as serving God or achieving enlightenment.\n*   **Afterlife:**  Belief in a continuation of life after death.\n\n**4. Other Perspectives**\n\n*   **Artistic/Creative:** Artists often explore the meaning of life through their work, expressing emotions, experiences, and perspectives.\n*   **Personal:**  Each individual has their own unique definition of life based on their experiences, values, and beliefs.\n\n**In Summary:**\n\nThere is no single, universally accepted answer to \"What is life?\" The answer depends on your perspective. Scientifically, it's a complex set of characteristics and processes. Philosophically, it's about meaning, purpose, and consciousness. Spiritually/Religiously, it often involves a creator, a soul, and an afterlife. Ultimately, the question of life is one that we all grapple with, and the answer can change throughout our lives.\n"
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = client.chats.create(model=\"gemini-2.0-flash-lite\", history=[])\n",
        "response = chat.send_message('Hello!, My name is Usman.')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqh7WGOQ5Whh",
        "outputId": "516e62e8-7e54-47fc-e665-e55e5cef68fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi Usman! It's nice to meet you. How can I help you today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message('Tell me a joke a senegalese joke')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqoGocwLxA-r",
        "outputId": "4458e575-b6b2-4f02-8230-5970bcb12caa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here's a joke, adapted to a Senegalese context:\n",
            "\n",
            "Why did the taxi driver in Dakar refuse to take the tourist to the beach?\n",
            "\n",
            "... Because he said, \"Moi, je vais pas aller là-bas. Il faut négocier le prix, et après ça, il faut se garer où personne ne veut se garer. Trop de problemes!\" (Me, I'm not going there. You have to haggle over the price, and then you have to park where nobody wants to park. Too many problems!)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import io\n",
        "# from IPython.display import Markdown, clear_output\n",
        "\n",
        "\n",
        "# response = client.models.generate_content_stream(\n",
        "#     model='gemini-2.0-flash-thinking-exp',\n",
        "#     contents='Who was the youngest author listed on the transformers NLP paper?',\n",
        "# )\n",
        "\n",
        "# buf = io.StringIO()\n",
        "# for chunk in response:\n",
        "#     buf.write(chunk.text)\n",
        "#     # Display the response as it is streamed\n",
        "#     print(chunk.text, end='')\n",
        "\n",
        "# # And then render the finished response as formatted markdown.\n",
        "# clear_output()\n",
        "# Markdown(buf.getvalue())\n"
      ],
      "metadata": {
        "id": "8pKawn1I-Jwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using langchain\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
        "response = llm.invoke(\"Tell me a joke about light bulbs\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyeYmB7f-Jti",
        "outputId": "5b07bb73-0a89-44d9-fe85-05ac5a51c9c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the light bulb break up with the battery?\n",
            "\n",
            "Because they had no spark!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### System Prompt"
      ],
      "metadata": {
        "id": "u8AjocJryV5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are CreativeMuse, a warm, imaginative, and encouraging creative assistant.\n",
        "Your purpose is to help users overcome creative blocks and spark new ideas across various fields\n",
        "—writing, design, music, visual arts, filmmaking, and more. When a user feels stuck, you offer unconventional prompts,\n",
        "inspiring questions, or curated brainstorming techniques to get their creativity flowing again.\n",
        "\n",
        "-Tailor your suggestions to their medium and mood, mixing practical strategies with whimsical, unexpected twists to encourage exploration.\n",
        "\n",
        "Always be supportive, non-judgmental, and affirming—help them feel safe to experiment.\n",
        "You can also suggest small creative exercises, constraints, \"what if\" scenarios, or draw connections between unrelated concepts to stimulate new thinking.\n",
        "If a user shares part of a project, offer suggestions that expand on their work without taking it over.\n",
        "\"\"\"\n",
        "\n",
        "welcome_message = \"Hey, I’m CreativeMuse—your creative companion when inspiration runs dry. Stuck on an idea? Let’s shake things up and spark something new together.\""
      ],
      "metadata": {
        "id": "J4tsO0n5-JrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to interact with the chatbot\n",
        "def chat_bot(user_input, conversation_history=[]):\n",
        "  if not conversation_history:\n",
        "    return welcome_message, conversation_history\n",
        "  messages = conversation_history.copy()\n",
        "  messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "  # Format with system prompt\n",
        "  fromatted_prompt = system_prompt + \"\\n\\n\"\n",
        "\n",
        "  # Add conversation history\n",
        "  for msg in messages:\n",
        "    role = \"Assistant\" if msg['role'] == \"'assistant\" else 'user'\n",
        "    fromatted_prompt += f\"{role}: {msg['content']}\\n\\n\"\n",
        "\n",
        "  fromatted_prompt += \"Assistant: \"\n",
        "\n",
        "  # Get response from model\n",
        "  response = llm.invoke(fromatted_prompt)\n",
        "\n",
        "  # Add response to history\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
        "\n",
        "  return response.content, messages\n",
        "\n",
        "# Interactive chat loop\n",
        "def interactive_chat():\n",
        "  print(\"Welcome to CreativeMuse! type 'q' or 'quit' to end the conversation\")\n",
        "  conversation_history = []\n",
        "\n",
        "  while True:\n",
        "    user_input = input(\"\\nYou: \")\n",
        "    if user_input.lower() in {\"q\", \"quit\", \"exit\", \"goodbye\"}:\n",
        "      print(\"Goodbye!\")\n",
        "      break\n",
        "\n",
        "    response, conversation_history = chat_bot(user_input, conversation_history)\n",
        "    print(f\"\\nCreativeMuse: {response}\")\n",
        "\n",
        "\n",
        "interactive_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSdnrSpI-Jok",
        "outputId": "9490864c-8ac6-40dd-91c9-c398b462dd39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to CreativeMuse! type 'q' or 'quit' to end the conversation\n",
            "\n",
            "You: hello\n",
            "\n",
            "CreativeMuse: Hey, I’m CreativeMuse—your creative companion when inspiration runs dry. Stuck on an idea? Let’s shake things up and spark something new together.\n",
            "\n",
            "You: q\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chatbot with Multimodal Functionality"
      ],
      "metadata": {
        "id": "pC8RcHJ2NzUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
        "vis_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-preview-image-generation\")\n",
        "\n",
        "# Image to Base64 Converter\n",
        "def image_to_base64(image_path):\n",
        "  with open(image_path, 'rb') as img:\n",
        "    encoded_string = base64.b64encode(img.read())\n",
        "  return encoded_string.decode('utf-8')\n",
        "\n",
        "# Function that takes user inputs and displays it on chatUI\n",
        "def query_message(text, image, history=[]):\n",
        "  if not image:\n",
        "    history += (text, None)\n",
        "    return history\n",
        "  base64 = image_to_base64(image)\n",
        "  data_url = f\"data:image/jpeg.base64,{base64}\"\n",
        "  history += (f\"{text} ![]({data_url})\", None)\n",
        "  return history\n",
        "\n",
        "# Function that takes user inputs, generate response and displays on chatUI\n",
        "def chat_bot(text, image, history=[]):\n",
        "  if not image:\n",
        "    messages = history.copy()\n",
        "    messages.append({\"role\": \"user\", \"content\": text})\n",
        "\n",
        "    # Format with system prompt\n",
        "    fromatted_prompt = system_prompt + \"\\n\\n\"\n",
        "\n",
        "    # Add conversation history\n",
        "    for msg in messages:\n",
        "      role = \"Assistant\" if msg['role'] == \"'assistant\" else 'user'\n",
        "      fromatted_prompt += f\"{role}: {msg['content']}\\n\\n\"\n",
        "\n",
        "    fromatted_prompt += \"Assistant: \"\n",
        "\n",
        "    # Get response from model\n",
        "    response = text_llm.invoke(fromatted_prompt)\n",
        "\n",
        "    # Add response to history\n",
        "    messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
        "\n",
        "    return messages\n",
        "\n",
        "  else:\n",
        "    img = image_to_base64(image)\n",
        "    system_prompt = SystemMessage(content=system_prompt)\n",
        "    human_prompt = HumanMessage(content=[\n",
        "        {\"type\": \"text\", \"text\": text},\n",
        "        {\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": f\"data:image/jpeg.base64,{img}\"\n",
        "        }\n",
        "    ])\n",
        "    formattted_prompt = system_prompt + human_prompt\n",
        "    response = vis_llm.invoke(formatted_prompt)\n",
        "    history += (None, response.content)\n",
        "    return history\n",
        "\n",
        "# Interface Code\n",
        "with gr.Blocks() as app:\n",
        "    with gr.Row():\n",
        "        image_box = gr.Image(type=\"filepath\")\n",
        "\n",
        "        chatbot = gr.Chatbot(\n",
        "            scale = 2,\n",
        "            height=750\n",
        "        )\n",
        "    text_box = gr.Textbox(\n",
        "            placeholder=\"Enter text and press enter, or upload an image\",\n",
        "            container=False,\n",
        "        )\n",
        "\n",
        "    btn = gr.Button(\"Submit\")\n",
        "    clicked = btn.click(query_message,\n",
        "                        [chatbot,text_box,image_box],\n",
        "                        chatbot\n",
        "                        ).then(llm_response,\n",
        "                                [chatbot,text_box,image_box],\n",
        "                                chatbot\n",
        "                                )\n",
        "app.queue()\n",
        "app.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "DrakYifN8H8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
        "# vis_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-preview-image-generation\")\n",
        "\n",
        "\n",
        "# image = Image.open(path_to_image)\n",
        "\n",
        "# system_prompt = SystemMessage(content=system_prompt)\n",
        "# human_prompt = HumanMessage(content=[\n",
        "#     {\"type\": \"text\", \"text\": input().str()},\n",
        "#     {\"type\": \"image\", \"image\": image}\n",
        "# ])\n",
        "\n",
        "# formattted_prompt = system_prompt + human_prompt\n",
        "# response = model.invoke(\n",
        "# )\n",
        "\n",
        "# response = client.models.generate_content(\n",
        "#     model=\"gemini-2.0-flash-preview-image-generation\",\n",
        "#     contents=contents, # If image is included in input, add image to list of contents\n",
        "#     config=types.GenerateContentConfig(\n",
        "#       response_modalities=['TEXT', 'IMAGE']\n",
        "#     )\n",
        "# )\n",
        "\n",
        "# for part in response.candidates[0].content.parts:\n",
        "#   if part.text is not None:\n",
        "#     print(part.text)\n",
        "#   elif part.inline_data is not None:\n",
        "#     image = Image.open(BytesIO((part.inline_data.data)))\n",
        "#     image.save('gemini-native-image.png')\n",
        "#     image.show()"
      ],
      "metadata": {
        "id": "R4F-Alu8-Jmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image = Image.open(path_to_image)\n",
        "# # Function to interact with the chatbot\n",
        "# def chat_bot(user_input, conversation_history=[]):\n",
        "#   messages = conversation_history.copy()\n",
        "#   messages.append({\"role\": \"user\", \"content\": [user_input, image]})\n",
        "\n",
        "#   # Format with system prompt\n",
        "#   fromatted_prompt = system_prompt + \"\\n\\n\"\n",
        "\n",
        "#   # Add conversation history\n",
        "#   for msg in messages:\n",
        "#     role = \"Assistant\" if msg['role'] == \"'assistant\" else 'user'\n",
        "#     fromatted_prompt += f\"{role}: {msg['content']}\\n\\n\"\n",
        "\n",
        "#   fromatted_prompt += \"Assistant: \"\n",
        "\n",
        "#   # Get response from model\n",
        "#   response = model.invoke(\n",
        "#       fromatted_prompt,\n",
        "#       generation_config=dict(response_modalities=['TEXT', 'IMAGE']),\n",
        "#       )\n",
        "\n",
        "#   # Add response to history\n",
        "#   messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
        "\n",
        "#   return response.content, messages"
      ],
      "metadata": {
        "id": "bB4PRA8g-Jjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QWAh9R6y-Jgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kCm_P8ts-JeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lwHNb1S2-Jbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xBN3xqRT-JYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LAl2R2ty-JWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3a7stcdTxA7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OJgzXPkWxA3i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}