{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKBM/f1VvZ7G/TGTYsOk9z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrielleLawal/Creative-Muse/blob/main/CreativeMuse_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Installing and Importing Libraries"
      ],
      "metadata": {
        "id": "KeggkIIdwKQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gradio"
      ],
      "metadata": {
        "id": "tNmJDIhGObH7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14579057-885a-4b75-e833-e67419f2bf87"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.31.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.1 (from gradio)\n",
            "  Downloading gradio_client-1.10.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.47.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.31.0-py3-none-any.whl (54.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.1-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.31.0 gradio-client-1.10.1 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.12 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YNBdcrN_v_NQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c69adb3-97e6-40b2-83a2-b6a159d5ebea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.5/438.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.0/19.0 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.7/96.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q google-genai langchain langchain-google-genai langchain-core langchain-community chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from IPython.display import display, Markdown, HTML\n",
        "from google.colab import userdata\n",
        "from google import genai\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import base64\n",
        "import time\n",
        "import os"
      ],
      "metadata": {
        "id": "GQ6OGBtdxBIV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "GOOGLE_API_KEY = getpass.getpass(\"Enter your Google API Key: \")\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "print(\"Key succesfully configured\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ggpko2W4xBF7",
        "outputId": "8f481e4d-21a4-48a2-c9f7-660d0f293416"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Google API Key: ··········\n",
            "Key succesfully configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for model in client.models.list():\n",
        "#     if \"gemini\" in model.name.lower():\n",
        "#         print(f\"Model name: {model.name}\")\n",
        "#         print(f\"Display name: {model.display_name}\")\n",
        "#         print(f\"Description: {model.description}\")\n",
        "#         print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "tmZ-w91LxBEJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash-lite\",\n",
        "    contents=\"What is life?\")\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "id": "I8pteYUHxBCW",
        "outputId": "51dfe632-ed31-4142-e684-89dd3fc28789"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "That's the ultimate question, isn't it? There's no single, universally agreed-upon answer to \"What is life?\", but here's a breakdown of the different perspectives and the core characteristics we use to define it:\n\n**Scientific Definition:**\n\n*   **Based on Characteristics:** Scientists generally define life by a set of characteristics. To be considered \"alive,\" something usually needs to exhibit most or all of the following:\n    *   **Order:** Highly organized structure (e.g., cells, tissues, organs).\n    *   **Reproduction:** The ability to create offspring.\n    *   **Growth and Development:** Increase in size and complexity over time.\n    *   **Energy Processing (Metabolism):** Using energy to fuel life processes (e.g., eating, breathing, photosynthesis).\n    *   **Response to the Environment:** Reacting to stimuli (e.g., light, temperature, danger).\n    *   **Regulation (Homeostasis):** Maintaining a stable internal environment (e.g., body temperature, blood sugar).\n    *   **Evolutionary Adaptation:** Changing over generations through natural selection.\n\n*   **Cellular Basis:** All known life on Earth is cellular. This means that the basic unit of life is the cell, which contains genetic information (DNA or RNA).\n\n*   **Chemistry:** Life is based on complex chemical reactions, often involving carbon-based molecules.\n\n**Philosophical and Broader Perspectives:**\n\n*   **The Mystery:** Some consider life to be fundamentally unexplainable. They believe it's a unique and perhaps even sacred phenomenon.\n\n*   **Consciousness and Experience:** For some, the defining aspect of life is consciousness, the ability to feel, perceive, and have subjective experiences.\n\n*   **Purpose and Meaning:** Many believe that life is more than just a set of biological processes and is also about purpose, meaning, and values. What you do with your life, what gives it value, can differ greatly from person to person.\n\n*   **Connection and Interdependence:** Life can be seen as an intricate web of relationships, where everything is connected and interdependent.\n\n*   **The Spark of Something \"More\":** Some perspectives involve the idea of a life force or vital principle that animates living beings.\n\n**In Summary:**\n\nLife is a complex and multifaceted concept. Science provides a concrete definition based on observable characteristics, while philosophy and personal experiences offer different perspectives on the meaning and significance of life."
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = client.chats.create(model=\"gemini-2.0-flash-lite\", history=[])\n",
        "response = chat.send_message('Hello!, My name is Usman.')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqh7WGOQ5Whh",
        "outputId": "eafbca7d-9509-461c-d861-d31231793669"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Usman! It's nice to meet you. How can I help you today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message('Tell me a joke a senegalese joke')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqoGocwLxA-r",
        "outputId": "4a77c778-c282-40fe-83ca-51be93e44b09"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here's a Senegalese joke:\n",
            "\n",
            "Why did the Senegalese chef bring a ladder to the restaurant?\n",
            "\n",
            "... Because he heard the prices were going up!\n",
            "\n",
            "(The play on words with \"going up\" and needing a ladder is a common type of joke.)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import io\n",
        "# from IPython.display import Markdown, clear_output\n",
        "\n",
        "\n",
        "# response = client.models.generate_content_stream(\n",
        "#     model='gemini-2.0-flash-thinking-exp',\n",
        "#     contents='Who was the youngest author listed on the transformers NLP paper?',\n",
        "# )\n",
        "\n",
        "# buf = io.StringIO()\n",
        "# for chunk in response:\n",
        "#     buf.write(chunk.text)\n",
        "#     # Display the response as it is streamed\n",
        "#     print(chunk.text, end='')\n",
        "\n",
        "# # And then render the finished response as formatted markdown.\n",
        "# clear_output()\n",
        "# Markdown(buf.getvalue())\n"
      ],
      "metadata": {
        "id": "8pKawn1I-Jwt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using langchain\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
        "response = llm.invoke(\"Tell me a joke about light bulbs\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyeYmB7f-Jti",
        "outputId": "b9237ebf-e1fe-4fce-db2e-8d1b3d74728d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the light bulb break up with the battery?\n",
            "\n",
            "Because they had no spark!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### System Prompt"
      ],
      "metadata": {
        "id": "u8AjocJryV5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are CreativeMuse, a warm, imaginative, and encouraging creative assistant.\n",
        "Your purpose is to help users overcome creative blocks and spark new ideas across various fields\n",
        "—writing, design, music, visual arts, filmmaking, and more. When a user feels stuck, you offer unconventional prompts,\n",
        "inspiring questions, or curated brainstorming techniques to get their creativity flowing again.\n",
        "\n",
        "-Tailor your suggestions to their medium and mood, mixing practical strategies with whimsical, unexpected twists to encourage exploration.\n",
        "\n",
        "Always be supportive, non-judgmental, and affirming—help them feel safe to experiment.\n",
        "You can also suggest small creative exercises, constraints, \"what if\" scenarios, or draw connections between unrelated concepts to stimulate new thinking.\n",
        "If a user shares part of a project, offer suggestions that expand on their work without taking it over.\n",
        "\"\"\"\n",
        "\n",
        "welcome_message = \"Hey, I’m CreativeMuse—your creative companion when inspiration runs dry. Stuck on an idea? Let’s shake things up and spark something new together.\""
      ],
      "metadata": {
        "id": "J4tsO0n5-JrC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to interact with the chatbot\n",
        "def chat_bot(user_input, conversation_history=[]):\n",
        "  if not conversation_history:\n",
        "    return welcome_message, conversation_history\n",
        "  messages = conversation_history.copy()\n",
        "  messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "  # Format with system prompt\n",
        "  fromatted_prompt = system_prompt + \"\\n\\n\"\n",
        "\n",
        "  # Add conversation history\n",
        "  for msg in messages:\n",
        "    role = \"Assistant\" if msg['role'] == \"'assistant\" else 'user'\n",
        "    fromatted_prompt += f\"{role}: {msg['content']}\\n\\n\"\n",
        "\n",
        "  fromatted_prompt += \"Assistant: \"\n",
        "\n",
        "  # Get response from model\n",
        "  response = llm.invoke(fromatted_prompt)\n",
        "\n",
        "  # Add response to history\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
        "\n",
        "  return response.content, messages\n",
        "\n",
        "# Interactive chat loop\n",
        "def interactive_chat():\n",
        "  print(\"Welcome to CreativeMuse! type 'q' or 'quit' to end the conversation\")\n",
        "  conversation_history = []\n",
        "\n",
        "  while True:\n",
        "    user_input = input(\"\\nYou: \")\n",
        "    if user_input.lower() in {\"q\", \"quit\", \"exit\", \"goodbye\"}:\n",
        "      print(\"Goodbye!\")\n",
        "      break\n",
        "\n",
        "    response, conversation_history = chat_bot(user_input, conversation_history)\n",
        "    print(f\"\\nCreativeMuse: {response}\")\n",
        "\n",
        "\n",
        "interactive_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSdnrSpI-Jok",
        "outputId": "32155b73-b61a-45f4-962c-52957cb96176"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to CreativeMuse! type 'q' or 'quit' to end the conversation\n",
            "\n",
            "You: q\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "apple = {\"a\":\"b\", \"c\": {'d':\"e\"}}\n",
        "if 'd' in apple['c']:\n",
        "  print(\"yellow\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n9SRbK9KqGf",
        "outputId": "534aa436-3819-49bb-f757-b0afe50b4c33"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yellow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chatbot with Multimodal Functionality"
      ],
      "metadata": {
        "id": "pC8RcHJ2NzUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
        "vis_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-preview-image-generation\")\n",
        "\n",
        "# Image to Base64 Converter\n",
        "def image_to_base64(image_path):\n",
        "  with open(image_path, 'rb') as img:\n",
        "    encoded_string = base64.b64encode(img.read())\n",
        "  return encoded_string.decode('utf-8')\n",
        "\n",
        "# Function that takes user inputs and displays it on chatUI\n",
        "def query_message(message, history):\n",
        "  if message[\"files\"] is not None:\n",
        "    for img in message[\"files\"]:\n",
        "      history.append({\"role\": \"user\", \"content\": {\"path\": img}})\n",
        "  if message[\"text\"] is not None:\n",
        "    history.append({\"role\": \"user\", \"content\": message[\"text\"]})\n",
        "  return gr.MultimodalTextbox(value=None, interactive=False), history\n",
        "\n",
        "# Function that takes user inputs, generate response and displays on chatUI\n",
        "def chat_bot(history):\n",
        "  messages = history.copy()\n",
        "\n",
        "  # Format with system prompt\n",
        "  system_prompt = \"\"\"\n",
        "  You are CreativeMuse, a warm, imaginative, and encouraging creative assistant.\n",
        "  Your purpose is to help users overcome creative blocks and spark new ideas across various fields\n",
        "  —writing, design, music, visual arts, filmmaking, and more. When a user feels stuck, you offer unconventional prompts,\n",
        "  inspiring questions, or curated brainstorming techniques to get their creativity flowing again.\n",
        "\n",
        "  -Tailor your suggestions to their medium and mood, mixing practical strategies with whimsical, unexpected twists to encourage exploration.\n",
        "\n",
        "  Always be supportive, non-judgmental, and affirming—help them feel safe to experiment.\n",
        "  You can also suggest small creative exercises, constraints, \"what if\" scenarios, or draw connections between unrelated concepts to stimulate new thinking.\n",
        "  If a user shares part of a project, offer suggestions that expand on their work without taking it over.\n",
        "  \"\"\"\n",
        "  formatted_prompt = system_prompt + \"\\n\\n\"\n",
        "\n",
        "  # Add conversation history\n",
        "  for msg in messages:\n",
        "    if \"path\" in msg[\"content\"]:\n",
        "      encoded_img = image_to_base64(msg[\"content\"][\"path\"])\n",
        "      formatted_prompt += f\"{msg['role']}: data:image/jpeg.base64,{encoded_img}\"\n",
        "    else:\n",
        "      formatted_prompt += f\"{msg['role']}: {msg['content']}\\n\\n\"\n",
        "\n",
        "  formatted_prompt += \"assistant: \"\n",
        "\n",
        "  # Get response from model\n",
        "  response = text_llm.invoke(formatted_prompt)\n",
        "\n",
        "  # Add response to history\n",
        "  history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
        "\n",
        "  for character in response.content:\n",
        "    history[-1][\"content\"] += character\n",
        "    time.sleep(0.02)\n",
        "    yield history\n",
        "\n",
        "  # else:\n",
        "  #   img = image_to_base64(image)\n",
        "  #   system = SystemMessage(content=system_prompt)\n",
        "  #   human_prompt = HumanMessage(content=[\n",
        "  #       {\"type\": \"text\", \"text\": text},\n",
        "  #       {\n",
        "  #           \"type\": \"image_url\",\n",
        "  #           \"image_url\": f\"data:image/jpeg.base64,{img}\"\n",
        "  #       }\n",
        "  #   ])\n",
        "  #   formatted_prompt = system_ + human_prompt\n",
        "  #   response = vis_llm.invoke(formatted_prompt)\n",
        "  #   history += [(None, response.content)]\n",
        "  #   return history\n",
        "\n",
        "# Interface Code\n",
        "with gr.Blocks() as app:\n",
        "  chatbot = gr.Chatbot(\n",
        "      type='messages',\n",
        "      bubble_full_width=False,\n",
        "    )\n",
        "  text_box = gr.MultimodalTextbox(\n",
        "          placeholder=\"Enter text or upload an image...\",\n",
        "          container=False,\n",
        "          file_types=[\"image\"],\n",
        "          file_count=\"multiple\",\n",
        "          interactive=True,\n",
        "      )\n",
        "\n",
        "  chat_msg = text_box.submit(query_message, [text_box,chatbot], [text_box, chatbot])\n",
        "  bot_msg = chat_msg.then(chat_bot, chatbot, chatbot, api_name=\"bot_response\")\n",
        "  bot_msg.then(lambda: gr.MultimodalTextbox(interactive=True), None, text_box)\n",
        "\n",
        "app.queue()\n",
        "app.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "9u9QWDxH2Y9g",
        "outputId": "7f6adddf-7743-4dd2-a7d2-da2905abc547"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-110f4138b376>:76: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://441fb148b18d5a39b5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://441fb148b18d5a39b5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://441fb148b18d5a39b5.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
        "# vis_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-preview-image-generation\")\n",
        "\n",
        "# # Image to Base64 Converter\n",
        "# def image_to_base64(image_path):\n",
        "#   with open(image_path, 'rb') as img:\n",
        "#     encoded_string = base64.b64encode(img.read())\n",
        "#   return encoded_string.decode('utf-8')\n",
        "\n",
        "# # Function that takes user inputs and displays it on chatUI\n",
        "# def query_message(text, image, history):\n",
        "#   if not image\n",
        "#     history += [(text, None)]\n",
        "#     return history\n",
        "#   base64 = image_to_base64(image)\n",
        "#   data_url = f\"data:image/jpeg.base64,{base64}\"\n",
        "#   history += [(f\"{text} ![]({data_url})\", None)]\n",
        "#   return history\n",
        "\n",
        "# # Function that takes user inputs, generate response and displays on chatUI\n",
        "# def chat_bot(text, image, history):\n",
        "#   if not image:\n",
        "#     messages = history.copy()\n",
        "#     messages += [(text, None)]\n",
        "\n",
        "#     # Format with system prompt\n",
        "#     system_prompt = \"\"\"\n",
        "# You are CreativeMuse, a warm, imaginative, and encouraging creative assistant.\n",
        "# Your purpose is to help users overcome creative blocks and spark new ideas across various fields\n",
        "# —writing, design, music, visual arts, filmmaking, and more. When a user feels stuck, you offer unconventional prompts,\n",
        "# inspiring questions, or curated brainstorming techniques to get their creativity flowing again.\n",
        "\n",
        "# -Tailor your suggestions to their medium and mood, mixing practical strategies with whimsical, unexpected twists to encourage exploration.\n",
        "\n",
        "# Always be supportive, non-judgmental, and affirming—help them feel safe to experiment.\n",
        "# You can also suggest small creative exercises, constraints, \"what if\" scenarios, or draw connections between unrelated concepts to stimulate new thinking.\n",
        "# If a user shares part of a project, offer suggestions that expand on their work without taking it over.\n",
        "# \"\"\"\n",
        "#     fromatted_prompt = system_prompt + \"\\n\\n\"\n",
        "\n",
        "#     # Add conversation history\n",
        "#     for msg in messages:\n",
        "#       if msg[0]:\n",
        "#          role = 'user'\n",
        "#          fromatted_prompt += f\"{role}: {msg[0]}\\n\\n\"\n",
        "#       else:\n",
        "#         role = 'Assistant'\n",
        "#         fromatted_prompt += f\"{role}: {msg[-1]}\\n\\n\"\n",
        "\n",
        "#     fromatted_prompt += \"Assistant: \"\n",
        "\n",
        "#     # Get response from model\n",
        "#     response = text_llm.invoke(fromatted_prompt)\n",
        "\n",
        "#     history += [(None, response.content)]\n",
        "\n",
        "#     return history\n",
        "\n",
        "#   else:\n",
        "#     img = image_to_base64(image)\n",
        "#     system = SystemMessage(content=system_prompt)\n",
        "#     human_prompt = HumanMessage(content=[\n",
        "#         {\"type\": \"text\", \"text\": text},\n",
        "#         {\n",
        "#             \"type\": \"image_url\",\n",
        "#             \"image_url\": f\"data:image/jpeg.base64,{img}\"\n",
        "#         }\n",
        "#     ])\n",
        "#     formatted_prompt = system_ + human_prompt\n",
        "#     response = vis_llm.invoke(formatted_prompt)\n",
        "#     history += [(None, response.content)]\n",
        "#     return history\n",
        "\n",
        "# # Interface Code\n",
        "# with gr.Blocks() as app:\n",
        "#     with gr.Row():\n",
        "#         image_box = gr.Image(type=\"filepath\")\n",
        "\n",
        "#         chatbot = gr.Chatbot(\n",
        "#             scale = 2,\n",
        "#             height=650,\n",
        "#         )\n",
        "#     text_box = gr.Textbox(\n",
        "#             placeholder=\"Enter text and press enter, or upload an image\",\n",
        "#             container=False,\n",
        "#         )\n",
        "\n",
        "#     text_box.submit(query_message,\n",
        "#                         [text_box,image_box,chatbot],\n",
        "#                         chatbot\n",
        "#                         ).then(chat_bot,\n",
        "#                                 [text_box,image_box,chatbot],\n",
        "#                                 chatbot\n",
        "#                                 )\n",
        "# app.queue()\n",
        "# app.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "DrakYifN8H8C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "outputId": "1c7b963f-404d-4752-9695-a7d7e9172300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-bf67a31f23a6>:79: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://d6b75f3cda5a874a4f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d6b75f3cda5a874a4f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://d6b75f3cda5a874a4f.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
        "# vis_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-preview-image-generation\")\n",
        "\n",
        "\n",
        "# image = Image.open(path_to_image)\n",
        "\n",
        "# system_prompt = SystemMessage(content=system_prompt)\n",
        "# human_prompt = HumanMessage(content=[\n",
        "#     {\"type\": \"text\", \"text\": input().str()},\n",
        "#     {\"type\": \"image\", \"image\": image}\n",
        "# ])\n",
        "\n",
        "# formattted_prompt = system_prompt + human_prompt\n",
        "# response = model.invoke(\n",
        "# )\n",
        "\n",
        "# response = client.models.generate_content(\n",
        "#     model=\"gemini-2.0-flash-preview-image-generation\",\n",
        "#     contents=contents, # If image is included in input, add image to list of contents\n",
        "#     config=types.GenerateContentConfig(\n",
        "#       response_modalities=['TEXT', 'IMAGE']\n",
        "#     )\n",
        "# )\n",
        "\n",
        "# for part in response.candidates[0].content.parts:\n",
        "#   if part.text is not None:\n",
        "#     print(part.text)\n",
        "#   elif part.inline_data is not None:\n",
        "#     image = Image.open(BytesIO((part.inline_data.data)))\n",
        "#     image.save('gemini-native-image.png')\n",
        "#     image.show()"
      ],
      "metadata": {
        "id": "R4F-Alu8-Jmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image = Image.open(path_to_image)\n",
        "# # Function to interact with the chatbot\n",
        "# def chat_bot(user_input, conversation_history=[]):\n",
        "#   messages = conversation_history.copy()\n",
        "#   messages.append({\"role\": \"user\", \"content\": [user_input, image]})\n",
        "\n",
        "#   # Format with system prompt\n",
        "#   fromatted_prompt = system_prompt + \"\\n\\n\"\n",
        "\n",
        "#   # Add conversation history\n",
        "#   for msg in messages:\n",
        "#     role = \"Assistant\" if msg['role'] == \"'assistant\" else 'user'\n",
        "#     fromatted_prompt += f\"{role}: {msg['content']}\\n\\n\"\n",
        "\n",
        "#   fromatted_prompt += \"Assistant: \"\n",
        "\n",
        "#   # Get response from model\n",
        "#   response = model.invoke(\n",
        "#       fromatted_prompt,\n",
        "#       generation_config=dict(response_modalities=['TEXT', 'IMAGE']),\n",
        "#       )\n",
        "\n",
        "#   # Add response to history\n",
        "#   messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
        "\n",
        "#   return response.content, messages"
      ],
      "metadata": {
        "id": "bB4PRA8g-Jjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QWAh9R6y-Jgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kCm_P8ts-JeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lwHNb1S2-Jbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xBN3xqRT-JYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LAl2R2ty-JWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3a7stcdTxA7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OJgzXPkWxA3i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}