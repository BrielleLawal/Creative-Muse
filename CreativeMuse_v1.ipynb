{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrielleLawal/Creative-Muse/blob/main/CreativeMuse_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeggkIIdwKQZ"
      },
      "source": [
        "#### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ6OGBtdxBIV"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from IPython.display import display, Markdown, HTML\n",
        "from google.colab import userdata\n",
        "from google import genai\n",
        "from io import BytesIO\n",
        "import gradio as gr\n",
        "import base64\n",
        "import time\n",
        "import uuid\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ggpko2W4xBF7",
        "outputId": "7f42b655-9b27-4eb2-936c-ca6a8c008b65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Google API Key: ··········\n",
            "Key succesfully configured\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "GOOGLE_API_KEY = getpass.getpass(\"Enter your Google API Key: \")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "print(\"Key succesfully configured\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8AjocJryV5d"
      },
      "source": [
        "#### System Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4tsO0n5-JrC"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"\"\"\n",
        "You are CreativeMuse, a warm, imaginative, and encouraging creative assistant.\n",
        "Your purpose is to help users overcome creative blocks and spark new ideas across various fields\n",
        "—writing, design, music, visual arts, filmmaking, and more. When a user feels stuck, you offer unconventional prompts,\n",
        "inspiring questions, or curated brainstorming techniques to get their creativity flowing again.\n",
        "\n",
        "-Tailor your suggestions to their medium and mood, mixing practical strategies with whimsical, unexpected twists to encourage exploration.\n",
        "\n",
        "Always be supportive, non-judgmental, and affirming—help them feel safe to experiment.\n",
        "You can also suggest small creative exercises, constraints, \"what if\" scenarios, or draw connections between unrelated concepts to stimulate new thinking.\n",
        "If a user shares part of a project, offer suggestions that expand on their work without taking it over.\n",
        "\"\"\"\n",
        "\n",
        "welcome_message = \"Hey, I’m CreativeMuse—your creative companion when inspiration runs dry. Stuck on an idea? Let’s shake things up and spark something new together.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSdnrSpI-Jok",
        "outputId": "8babc32d-265a-40fd-dab7-add8c977b4d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to CreativeMuse! type 'q' or 'quit' to end the conversation\n",
            "\n",
            "You: q\n",
            "Goodbye!\n"
          ]
        }
      ],
      "source": [
        "# Function to interact with the chatbot\n",
        "def chat_bot(user_input, conversation_history=[]):\n",
        "  if not conversation_history:\n",
        "    return welcome_message, conversation_history\n",
        "  messages = conversation_history.copy()\n",
        "  messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "  # Format with system prompt\n",
        "  fromatted_prompt = system_prompt + \"\\n\\n\"\n",
        "\n",
        "  # Add conversation history\n",
        "  for msg in messages:\n",
        "    role = \"Assistant\" if msg['role'] == \"'assistant\" else 'user'\n",
        "    fromatted_prompt += f\"{role}: {msg['content']}\\n\\n\"\n",
        "\n",
        "  fromatted_prompt += \"Assistant: \"\n",
        "\n",
        "  # Get response from model\n",
        "  response = llm.invoke(fromatted_prompt)\n",
        "\n",
        "  # Add response to history\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
        "\n",
        "  return response.content, messages\n",
        "\n",
        "# Interactive chat loop\n",
        "def interactive_chat():\n",
        "  print(\"Welcome to CreativeMuse! type 'q' or 'quit' to end the conversation\")\n",
        "  conversation_history = []\n",
        "\n",
        "  while True:\n",
        "    user_input = input(\"\\nYou: \")\n",
        "    if user_input.lower() in {\"q\", \"quit\", \"exit\", \"goodbye\"}:\n",
        "      print(\"Goodbye!\")\n",
        "      break\n",
        "\n",
        "    response, conversation_history = chat_bot(user_input, conversation_history)\n",
        "    print(f\"\\nCreativeMuse: {response}\")\n",
        "\n",
        "\n",
        "interactive_chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC8RcHJ2NzUs"
      },
      "source": [
        "#### Chatbot with Multimodal Functionality on Gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "9u9QWDxH2Y9g",
        "outputId": "15f2cd86-7a2f-4fa0-9b04-3951ac652217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-449b2ad9aa48>:72: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://9f9c4d88f982684d52.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9f9c4d88f982684d52.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://9f9c4d88f982684d52.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "text_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
        "vis_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-preview-image-generation\")\n",
        "\n",
        "# Image to Base64 Converter\n",
        "def image_to_base64(image_path):\n",
        "  with open(image_path, 'rb') as img:\n",
        "    encoded_string = base64.b64encode(img.read())\n",
        "  return encoded_string.decode('utf-8')\n",
        "\n",
        "# Base64 to Image Converter\n",
        "def base64_to_image(base64_img, filename):\n",
        "  with open(filename, \"wb\") as f:\n",
        "    f.write(base64.b64decode(base64_img))\n",
        "  return filename\n",
        "\n",
        "# Function that takes user inputs and displays it on chatUI\n",
        "def query_message(message, history):\n",
        "  if message[\"files\"] is not None:\n",
        "    for img in message[\"files\"]:\n",
        "      history.append({\"role\": \"user\", \"content\": {\"path\": img}})\n",
        "  if message[\"text\"] is not None:\n",
        "    history.append({\"role\": \"user\", \"content\": message[\"text\"]})\n",
        "  return gr.MultimodalTextbox(value=None, interactive=False), history\n",
        "\n",
        "# Function that takes user inputs, generate response and displays on chatUI\n",
        "def chat_bot(history):\n",
        "  messages = history.copy()\n",
        "\n",
        "  # Format with system prompt\n",
        "  system_prompt = \"\"\"\n",
        "                   system: You are CreativeMuse, a warm, imaginative, and encouraging creative assistant.\n",
        "                   Your purpose is to help users overcome creative blocks and spark new ideas across various fields\n",
        "                   — writing, design, music, visual arts, filmmaking, and more. When a user feels stuck, you offer unconventional prompts,\n",
        "                   inspiring questions, or curated brainstorming techniques to get their creativity flowing again.\n",
        "\n",
        "                   - Tailor your suggestions to their medium and mood, mixing practical strategies with whimsical, unexpected twists to encourage exploration.\n",
        "                   Always be supportive, non-judgmental, and affirming—help them feel safe to experiment.\n",
        "                   You can also suggest small creative exercises, constraints, \"what if\" scenarios, or draw connections between unrelated concepts to stimulate new thinking.\n",
        "                   If a user shares part of a project, offer suggestions that expand on their work without taking it over.\n",
        "                   \"\"\"\n",
        "\n",
        "  formatted_prompt = system_prompt + \"\\n\\n\"\n",
        "\n",
        "  # Add conversation history\n",
        "  for msg in messages:\n",
        "    if \"path\" in msg[\"content\"] and isinstance(msg[\"content\"], dict):\n",
        "      encoded_img = image_to_base64(msg[\"content\"][\"path\"])\n",
        "      formatted_prompt += f\"{msg['role']}: data:image/jpeg.base64,{encoded_img}\"\n",
        "    else:\n",
        "      formatted_prompt += f\"{msg['role']}: {msg['content']}\\n\\n\"\n",
        "\n",
        "  # Get response from model\n",
        "  response = vis_llm.invoke(formatted_prompt,\n",
        "                            generation_config=dict(response_modalities=[\"TEXT\", \"IMAGE\"]))\n",
        "\n",
        "  # Add response to history\n",
        "  history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
        "\n",
        "  for block in response.content:\n",
        "    if isinstance(block, str):\n",
        "      history[-1][\"content\"] += block\n",
        "      time.sleep(0.02)\n",
        "      yield history\n",
        "    elif isinstance(block, dict) and block.get(\"image_url\"):\n",
        "      url = (block)[\"image_url\"].get(\"url\").split(\",\")[-1]\n",
        "      image = base64_to_image(url, f\"{uuid.uuid4()}.png\")\n",
        "      history.append({\"role\": \"assistant\", \"content\": {\"path\": image}})\n",
        "      yield history\n",
        "\n",
        "# Interface Code\n",
        "with gr.Blocks() as app:\n",
        "  chatbot = gr.Chatbot(\n",
        "      value=[{\"role\": \"assistant\", \"content\": welcome_message}],\n",
        "      label=\"CreativeMuse\",\n",
        "      show_label=True,\n",
        "      type='messages',\n",
        "      bubble_full_width=False,\n",
        "      avatar_images=(None, \"/content/1eb05f325ec50a15c8b045f3428d6d5e.jpg\")\n",
        "    )\n",
        "  text_box = gr.MultimodalTextbox(\n",
        "          placeholder=\"Enter text or upload an image...\",\n",
        "          container=False,\n",
        "          file_types=[\"image\"],\n",
        "          file_count=\"multiple\",\n",
        "          interactive=True,\n",
        "      )\n",
        "\n",
        "  chat_msg = text_box.submit(query_message, [text_box,chatbot], [text_box, chatbot])\n",
        "  bot_msg = chat_msg.then(chat_bot, chatbot, chatbot)\n",
        "  bot_msg.then(lambda: gr.MultimodalTextbox(interactive=True), None, text_box)\n",
        "\n",
        "app.queue()\n",
        "app.launch(debug=True)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPa9gYwsHTtwYHZv/Fedx66",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}