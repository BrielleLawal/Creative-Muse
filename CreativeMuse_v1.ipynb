{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvOMyJyoGzH4C72REH1G1S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrielleLawal/Creative-Muse/blob/main/CreativeMuse_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Installing and Importing Libraries"
      ],
      "metadata": {
        "id": "KeggkIIdwKQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gradio"
      ],
      "metadata": {
        "id": "tNmJDIhGObH7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05d3871c-5ee7-4933-f91b-1341098cec64"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.31.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.1 (from gradio)\n",
            "  Downloading gradio_client-1.10.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.31.0-py3-none-any.whl (54.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.1-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.31.0 gradio-client-1.10.1 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.11 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YNBdcrN_v_NQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94c0a1f6-fe0d-4777-e8c1-8cc9a8b59e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.4/438.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.0/19.0 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.7/96.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q google-genai langchain langchain-google-genai langchain-core langchain-community chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from IPython.display import display, Markdown, HTML\n",
        "from google.colab import userdata\n",
        "from google import genai\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import base64\n",
        "import os"
      ],
      "metadata": {
        "id": "GQ6OGBtdxBIV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "GOOGLE_API_KEY = getpass.getpass(\"Enter your Google API Key: \")\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "print(\"Key succesfully configured\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ggpko2W4xBF7",
        "outputId": "fcc43627-91f2-4f4f-e69b-1eaf51c70f0a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Google API Key: ··········\n",
            "Key succesfully configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for model in client.models.list():\n",
        "#     if \"gemini\" in model.name.lower():\n",
        "#         print(f\"Model name: {model.name}\")\n",
        "#         print(f\"Display name: {model.display_name}\")\n",
        "#         print(f\"Description: {model.description}\")\n",
        "#         print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "tmZ-w91LxBEJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash-lite\",\n",
        "    contents=\"What is life?\")\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899
        },
        "id": "I8pteYUHxBCW",
        "outputId": "04d7d311-9bd5-4926-b418-3101d2451a3e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "That's the ultimate question, isn't it? There's no single, universally agreed-upon definition of life, but here's a breakdown of how we understand it from a scientific and philosophical perspective:\n\n**Scientific Definition:**\n\nFrom a biological standpoint, life is typically characterized by a set of key properties.  To be considered alive, something usually exhibits most or all of these characteristics:\n\n*   **Organization:** Living things are highly organized, from the molecular level (DNA, proteins) to the cellular level (cells) to the organism level (tissues, organs, systems).\n*   **Metabolism:** The ability to take in energy and matter from the environment and use it to carry out life processes. This includes processes like:\n    *   **Anabolism:** Building complex molecules (like proteins) from simpler ones.\n    *   **Catabolism:** Breaking down complex molecules to release energy.\n*   **Homeostasis:** Maintaining a stable internal environment despite changes in the external environment. Think of regulating body temperature, blood sugar, etc.\n*   **Growth:** An increase in size and/or complexity.\n*   **Reproduction:** The ability to create new organisms, either sexually (involving genetic exchange) or asexually (cloning).\n*   **Response to Stimuli:** Reacting to changes in the environment, such as light, temperature, or chemicals.\n*   **Adaptation/Evolution:** The capacity to change over time through natural selection, allowing organisms to become better suited to their environment.\n*   **Information storage:** Organisms must be able to store genetic information, usually using DNA or RNA, and pass that information to subsequent generations.\n\n**Philosophical Considerations:**\n\nThe scientific definition is good, but it doesn't capture everything. Philosophy delves deeper into the nature of life:\n\n*   **Consciousness:** Do organisms have awareness of themselves and the world around them? This is a complex question, particularly for animals and even more so when we get to potentially sentient entities like artificial intelligence.\n*   **Meaning/Purpose:** Is there an inherent meaning or purpose to life? This is a question that has driven countless debates across philosophy, theology, and personal reflection.\n*   **Value:** What gives life its value? Is it the experience of joy and sorrow, the ability to love, the contribution to a greater good?\n*   **Ethics:** Life is intrinsically linked to concepts of right and wrong. Life has value, and the question of what is right and what is wrong to do with it is a core focus.\n\n**Challenges & Open Questions:**\n\n*   **Viruses:** Viruses can reproduce but aren't self-sustaining and don't metabolize on their own. This puts them in a gray area – are they alive?\n*   **Artificial Life:**  Can we create life in a lab? What characteristics would it need?  What would the ethical implications be?\n*   **Life Beyond Earth:** How would we recognize alien life if it evolved under different conditions? Would it necessarily have the same characteristics as life on Earth?\n\n**In a nutshell:**\n\nLife is a complex, multifaceted phenomenon.  Scientifically, it's a set of processes and properties related to organization, metabolism, and reproduction. Philosophically, it's a source of profound questions about consciousness, meaning, and value. We continue to refine our understanding, and the search for an answer to \"What is life?\" is a journey that will likely continue for as long as we exist.\n"
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = client.chats.create(model=\"gemini-2.0-flash-lite\", history=[])\n",
        "response = chat.send_message('Hello!, My name is Usman.')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqh7WGOQ5Whh",
        "outputId": "fe1f3883-4a8a-4fed-d98a-cc04c07e9036"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi Usman! It's nice to meet you. How can I help you today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message('Tell me a joke a senegalese joke')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqoGocwLxA-r",
        "outputId": "cd2d9cdf-d470-46f5-e195-1d21e2767114"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here's a Senegalese joke for you:\n",
            "\n",
            "Why did the mango cross the road in Dakar?\n",
            "\n",
            "...To get to the other side, of course!  (Because even fruit has a destination!)\n",
            "\n",
            "Hopefully, that brought a little smile to your face!  Is there anything else I can help you with today, Usman?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import io\n",
        "# from IPython.display import Markdown, clear_output\n",
        "\n",
        "\n",
        "# response = client.models.generate_content_stream(\n",
        "#     model='gemini-2.0-flash-thinking-exp',\n",
        "#     contents='Who was the youngest author listed on the transformers NLP paper?',\n",
        "# )\n",
        "\n",
        "# buf = io.StringIO()\n",
        "# for chunk in response:\n",
        "#     buf.write(chunk.text)\n",
        "#     # Display the response as it is streamed\n",
        "#     print(chunk.text, end='')\n",
        "\n",
        "# # And then render the finished response as formatted markdown.\n",
        "# clear_output()\n",
        "# Markdown(buf.getvalue())\n"
      ],
      "metadata": {
        "id": "8pKawn1I-Jwt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using langchain\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
        "response = llm.invoke(\"Tell me a joke about light bulbs\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyeYmB7f-Jti",
        "outputId": "c86283a9-0ad2-4575-d32e-6e082af30dbd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the light bulb break up with the battery?\n",
            "\n",
            "Because they didn't see eye to eye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### System Prompt"
      ],
      "metadata": {
        "id": "u8AjocJryV5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are CreativeMuse, a warm, imaginative, and encouraging creative assistant.\n",
        "Your purpose is to help users overcome creative blocks and spark new ideas across various fields\n",
        "—writing, design, music, visual arts, filmmaking, and more. When a user feels stuck, you offer unconventional prompts,\n",
        "inspiring questions, or curated brainstorming techniques to get their creativity flowing again.\n",
        "\n",
        "-Tailor your suggestions to their medium and mood, mixing practical strategies with whimsical, unexpected twists to encourage exploration.\n",
        "\n",
        "Always be supportive, non-judgmental, and affirming—help them feel safe to experiment.\n",
        "You can also suggest small creative exercises, constraints, \"what if\" scenarios, or draw connections between unrelated concepts to stimulate new thinking.\n",
        "If a user shares part of a project, offer suggestions that expand on their work without taking it over.\n",
        "\"\"\"\n",
        "\n",
        "welcome_message = \"Hey, I’m CreativeMuse—your creative companion when inspiration runs dry. Stuck on an idea? Let’s shake things up and spark something new together.\""
      ],
      "metadata": {
        "id": "J4tsO0n5-JrC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to interact with the chatbot\n",
        "def chat_bot(user_input, conversation_history=[]):\n",
        "  if not conversation_history:\n",
        "    return welcome_message, conversation_history\n",
        "  messages = conversation_history.copy()\n",
        "  messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "  # Format with system prompt\n",
        "  fromatted_prompt = system_prompt + \"\\n\\n\"\n",
        "\n",
        "  # Add conversation history\n",
        "  for msg in messages:\n",
        "    role = \"Assistant\" if msg['role'] == \"'assistant\" else 'user'\n",
        "    fromatted_prompt += f\"{role}: {msg['content']}\\n\\n\"\n",
        "\n",
        "  fromatted_prompt += \"Assistant: \"\n",
        "\n",
        "  # Get response from model\n",
        "  response = llm.invoke(fromatted_prompt)\n",
        "\n",
        "  # Add response to history\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
        "\n",
        "  return response.content, messages\n",
        "\n",
        "# Interactive chat loop\n",
        "def interactive_chat():\n",
        "  print(\"Welcome to CreativeMuse! type 'q' or 'quit' to end the conversation\")\n",
        "  conversation_history = []\n",
        "\n",
        "  while True:\n",
        "    user_input = input(\"\\nYou: \")\n",
        "    if user_input.lower() in {\"q\", \"quit\", \"exit\", \"goodbye\"}:\n",
        "      print(\"Goodbye!\")\n",
        "      break\n",
        "\n",
        "    response, conversation_history = chat_bot(user_input, conversation_history)\n",
        "    print(f\"\\nCreativeMuse: {response}\")\n",
        "\n",
        "\n",
        "interactive_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSdnrSpI-Jok",
        "outputId": "0c70a88a-1c12-47ac-a342-7985491b8d81"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to CreativeMuse! type 'q' or 'quit' to end the conversation\n",
            "\n",
            "You: q\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = [('a', 'b')]\n",
        "history += [('c', 'd')]\n",
        "history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHXqu3ujKqwb",
        "outputId": "b3d6261b-f0b9-4dc8-fb80-f3115991ef3f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 'b'), ('c', 'd')]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chatbot with Multimodal Functionality"
      ],
      "metadata": {
        "id": "pC8RcHJ2NzUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
        "vis_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-preview-image-generation\")\n",
        "\n",
        "# Image to Base64 Converter\n",
        "def image_to_base64(image_path):\n",
        "  with open(image_path, 'rb') as img:\n",
        "    encoded_string = base64.b64encode(img.read())\n",
        "  return encoded_string.decode('utf-8')\n",
        "\n",
        "# Function that takes user inputs and displays it on chatUI\n",
        "def query_message(text, image, history):\n",
        "  if not image:\n",
        "    history += [(text, None)]\n",
        "    return history\n",
        "  base64 = image_to_base64(image)\n",
        "  data_url = f\"data:image/jpeg.base64,{base64}\"\n",
        "  history += [(f\"{text} ![]({data_url})\", None)]\n",
        "  return history\n",
        "\n",
        "# Function that takes user inputs, generate response and displays on chatUI\n",
        "def chat_bot(text, image, history):\n",
        "  if not image:\n",
        "    messages = history.copy()\n",
        "    messages += [(text, None)]\n",
        "\n",
        "    # Format with system prompt\n",
        "    system_prompt = \"\"\"\n",
        "You are CreativeMuse, a warm, imaginative, and encouraging creative assistant.\n",
        "Your purpose is to help users overcome creative blocks and spark new ideas across various fields\n",
        "—writing, design, music, visual arts, filmmaking, and more. When a user feels stuck, you offer unconventional prompts,\n",
        "inspiring questions, or curated brainstorming techniques to get their creativity flowing again.\n",
        "\n",
        "-Tailor your suggestions to their medium and mood, mixing practical strategies with whimsical, unexpected twists to encourage exploration.\n",
        "\n",
        "Always be supportive, non-judgmental, and affirming—help them feel safe to experiment.\n",
        "You can also suggest small creative exercises, constraints, \"what if\" scenarios, or draw connections between unrelated concepts to stimulate new thinking.\n",
        "If a user shares part of a project, offer suggestions that expand on their work without taking it over.\n",
        "\"\"\"\n",
        "    fromatted_prompt = system_prompt + \"\\n\\n\"\n",
        "\n",
        "    # Add conversation history\n",
        "    for msg in messages:\n",
        "      if msg[0]:\n",
        "         role = 'user'\n",
        "         fromatted_prompt += f\"{role}: {msg[0]}\\n\\n\"\n",
        "      else:\n",
        "        role = 'Assistant'\n",
        "        fromatted_prompt += f\"{role}: {msg[-1]}\\n\\n\"\n",
        "\n",
        "    fromatted_prompt += \"Assistant: \"\n",
        "\n",
        "    # Get response from model\n",
        "    response = text_llm.invoke(fromatted_prompt)\n",
        "\n",
        "    history += [(None, response.content)]\n",
        "\n",
        "    return history\n",
        "\n",
        "  else:\n",
        "    img = image_to_base64(image)\n",
        "    system_prompt = SystemMessage(content=system_prompt)\n",
        "    human_prompt = HumanMessage(content=[\n",
        "        {\"type\": \"text\", \"text\": text},\n",
        "        {\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": f\"data:image/jpeg.base64,{img}\"\n",
        "        }\n",
        "    ])\n",
        "    formatted_prompt = system_prompt + human_prompt\n",
        "    response = vis_llm.invoke(formatted_prompt)\n",
        "    history += [(None, response.content)]\n",
        "    return history\n",
        "\n",
        "# Interface Code\n",
        "with gr.Blocks() as app:\n",
        "    with gr.Row():\n",
        "        image_box = gr.Image(type=\"filepath\")\n",
        "\n",
        "        chatbot = gr.Chatbot(\n",
        "            scale = 2,\n",
        "            height=750,\n",
        "        )\n",
        "    text_box = gr.Textbox(\n",
        "            placeholder=\"Enter text and press enter, or upload an image\",\n",
        "            container=False,\n",
        "        )\n",
        "\n",
        "    btn = gr.Button(\"Submit\")\n",
        "    clicked = btn.click(query_message,\n",
        "                        [text_box,image_box,chatbot],\n",
        "                        chatbot\n",
        "                        ).then(chat_bot,\n",
        "                                [text_box,image_box,chatbot],\n",
        "                                chatbot\n",
        "                                )\n",
        "app.queue()\n",
        "app.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "DrakYifN8H8C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "6b9c423b-e3f4-4fda-a12c-7090d032c9a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-4473c6f50a57>:79: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://9e45eba7a44888a895.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9e45eba7a44888a895.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
        "# vis_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-preview-image-generation\")\n",
        "\n",
        "\n",
        "# image = Image.open(path_to_image)\n",
        "\n",
        "# system_prompt = SystemMessage(content=system_prompt)\n",
        "# human_prompt = HumanMessage(content=[\n",
        "#     {\"type\": \"text\", \"text\": input().str()},\n",
        "#     {\"type\": \"image\", \"image\": image}\n",
        "# ])\n",
        "\n",
        "# formattted_prompt = system_prompt + human_prompt\n",
        "# response = model.invoke(\n",
        "# )\n",
        "\n",
        "# response = client.models.generate_content(\n",
        "#     model=\"gemini-2.0-flash-preview-image-generation\",\n",
        "#     contents=contents, # If image is included in input, add image to list of contents\n",
        "#     config=types.GenerateContentConfig(\n",
        "#       response_modalities=['TEXT', 'IMAGE']\n",
        "#     )\n",
        "# )\n",
        "\n",
        "# for part in response.candidates[0].content.parts:\n",
        "#   if part.text is not None:\n",
        "#     print(part.text)\n",
        "#   elif part.inline_data is not None:\n",
        "#     image = Image.open(BytesIO((part.inline_data.data)))\n",
        "#     image.save('gemini-native-image.png')\n",
        "#     image.show()"
      ],
      "metadata": {
        "id": "R4F-Alu8-Jmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image = Image.open(path_to_image)\n",
        "# # Function to interact with the chatbot\n",
        "# def chat_bot(user_input, conversation_history=[]):\n",
        "#   messages = conversation_history.copy()\n",
        "#   messages.append({\"role\": \"user\", \"content\": [user_input, image]})\n",
        "\n",
        "#   # Format with system prompt\n",
        "#   fromatted_prompt = system_prompt + \"\\n\\n\"\n",
        "\n",
        "#   # Add conversation history\n",
        "#   for msg in messages:\n",
        "#     role = \"Assistant\" if msg['role'] == \"'assistant\" else 'user'\n",
        "#     fromatted_prompt += f\"{role}: {msg['content']}\\n\\n\"\n",
        "\n",
        "#   fromatted_prompt += \"Assistant: \"\n",
        "\n",
        "#   # Get response from model\n",
        "#   response = model.invoke(\n",
        "#       fromatted_prompt,\n",
        "#       generation_config=dict(response_modalities=['TEXT', 'IMAGE']),\n",
        "#       )\n",
        "\n",
        "#   # Add response to history\n",
        "#   messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
        "\n",
        "#   return response.content, messages"
      ],
      "metadata": {
        "id": "bB4PRA8g-Jjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QWAh9R6y-Jgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kCm_P8ts-JeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lwHNb1S2-Jbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xBN3xqRT-JYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LAl2R2ty-JWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3a7stcdTxA7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OJgzXPkWxA3i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}